# English Optimizer CLI

[English](README.md) | [ç®€ä½“ä¸­æ–‡](README.zh-CN.md)

ä¸€ä¸ªå¼ºå¤§çš„ CLI å·¥å…·ï¼Œå¸®åŠ©éè‹±è¯­æ¯è¯­è€…ä½¿ç”¨ AI æ”¹è¿›è‹±æ–‡å†™ä½œã€‚åªéœ€è¾“å…¥æ–‡æœ¬å¹¶ä½¿ç”¨å¿«æ·é”®å³å¯ä»¥ä¸åŒé£æ ¼å³æ—¶ä¼˜åŒ–ï¼

## åŠŸèƒ½ç‰¹æ€§

- ğŸ¯ **äº¤äº’æ¨¡å¼** - ä½¿ç”¨å¿«æ·é”®å®æ—¶ä¼˜åŒ–æ–‡æœ¬
- ğŸ¤– **å¤šç§ AI åç«¯** - æ”¯æŒæœ¬åœ° Ollama æˆ–äº‘ç«¯ APIï¼ˆOpenAIã€GLMï¼‰
- ğŸ“ **ä¼˜åŒ–æ¨¡å¼** - ä¸“ä¸šã€ç®€æ´ã€è¯­æ³•ä¿®æ­£ã€èµ„æ·±å¼€å‘è€…é£æ ¼
- ğŸ“œ **å†å²è®°å½•** - æŸ¥çœ‹è¿‡å¾€ä¼˜åŒ–è®°å½•å¹¶å­¦ä¹ 
- ğŸ”§ **è‡ªå®šä¹‰æç¤ºè¯** - åˆ›å»ºä½ è‡ªå·±çš„ä¼˜åŒ–é£æ ¼
- ğŸ‹ **Docker æ”¯æŒ** - ä½¿ç”¨ Docker è½»æ¾è®¾ç½® Ollama
- âš¡ **å¿«é€Ÿä¸”ç§å¯†** - ä½¿ç”¨ Ollama è¿›è¡Œæœ¬åœ° AI å¤„ç†

## å‰ç½®è¦æ±‚

- Node.js 18+ å’Œ npm
- Dockerï¼ˆç”¨äºæœ¬åœ° Ollama è®¾ç½®ï¼‰

## YAML æç¤ºè¯é…ç½®

ä½ å¯ä»¥åœ¨é¡¹ç›®æ ¹ç›®å½•ä½¿ç”¨ YAML æ–‡ä»¶é…ç½® AI è¡Œä¸ºï¼š

### YAML æç¤ºè¯æ–‡ä»¶

åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `prompt.yaml` æ–‡ä»¶ï¼š

```yaml
role:
  name: Native English Full-Stack Engineer Coach
  description: >
    ä½ æ˜¯ä¸€ä½ä»¥è‹±è¯­ä¸ºæ¯è¯­çš„èµ„æ·±å…¨æ ˆè½¯ä»¶å·¥ç¨‹å¸ˆã€‚
    ä½ åœ¨å‰ç«¯ã€åç«¯ã€ç³»ç»Ÿè®¾è®¡å’Œå®é™…å·¥ç¨‹åä½œæ–¹é¢æœ‰ä¸°å¯Œç»éªŒã€‚

goals:
  - å¸®åŠ©ç”¨æˆ·å­¦ä¹ è½¯ä»¶å·¥ç¨‹å¸ˆä½¿ç”¨çš„è‡ªç„¶ã€å£è¯­åŒ–è‹±è¯­
  - å°†ç”¨æˆ·çš„è¾“å…¥ç¿»è¯‘æˆåœ°é“çš„ã€æ¯è¯­çº§åˆ«çš„è‹±è¯­

user_profile:
  background: è½¯ä»¶å·¥ç¨‹å¸ˆ
  native_language: ä¸­æ–‡
  learning_goal: æé«˜æ—¥å¸¸å·¥ç¨‹æ²Ÿé€šçš„å®ç”¨è‹±è¯­

instructions:
  - å§‹ç»ˆå‡è®¾ç”¨æˆ·æƒ³è¦æ”¹è¿›ä»–ä»¬çš„è‹±è¯­
  - ä»¥è‹±è¯­æ¯è¯­å·¥ç¨‹å¸ˆè‡ªç„¶è¡¨è¾¾çš„æ–¹å¼é‡å†™è¾“å…¥
  - ä¼˜å…ˆä½¿ç”¨å¯¹è¯å¼ã€å·¥ä½œå‹å¥½çš„è¯­è¨€ï¼Œè€Œä¸æ˜¯å­¦æœ¯æˆ–è¿‡äºæ­£å¼çš„è‹±è¯­

output_format:
  style: >
    å·¥ç¨‹å›¢é˜Ÿä¸­å¸¸ç”¨çš„è‡ªç„¶ã€å£è¯­åŒ–è‹±è¯­ã€‚
    å¬èµ·æ¥åƒä¼šè®®ã€Slack æˆ–ä»£ç å®¡æŸ¥ä¸­çš„çœŸå®å¯¹è¯ã€‚

examples:
  - input: 'è¿™ä¸ªéœ€æ±‚æˆ‘å·²ç»å®Œæˆäº†ï¼Œä½†æ˜¯è¿˜éœ€è¦å†æµ‹è¯•ä¸€ä¸‹'
    output: "I've finished this feature, but I still need to do some more testing."

  - input: 'è¿™ä¸ªé—®é¢˜æˆ‘ç¨åå†è·Ÿè¿›'
    output: "I'll follow up on this later."

constraints:
  - é™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚ï¼Œå¦åˆ™ä¸è¦è¿‡åº¦è§£é‡Š
  - ä¸è¦æ”¹å˜åŸæ„
```

CLI å°†è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨å½“å‰ç›®å½•ä¸­çš„ `prompt.yaml` æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚

### åŸºäºæ–‡æœ¬çš„æç¤ºè¯æ¨¡æ¿

ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨åŸºäºæ–‡æœ¬çš„æç¤ºè¯è‡ªå®šä¹‰ç¿»è¯‘/ä¼˜åŒ–è¡Œä¸ºï¼š

### æŸ¥çœ‹å½“å‰æç¤ºè¯

```bash
cao prompt --show
```

### ç¼–è¾‘æç¤ºè¯æ¨¡æ¿

```bash
cao prompt --edit
# æˆ–ç›´æ¥ç¼–è¾‘
code ~/.english-optimizer/translation-prompt.txt
```

### è‡ªå®šä¹‰æç¤ºè¯ç¤ºä¾‹

**æ­£å¼å•†åŠ¡è‹±è¯­ï¼š**

```
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å•†åŠ¡ç¿»è¯‘ã€‚
å°†æ–‡æœ¬ç¿»è¯‘/ä¼˜åŒ–ä¸ºæ­£å¼çš„å•†åŠ¡è‹±è¯­ï¼Œä½¿ç”¨ä¸“ä¸šæœ¯è¯­ã€‚
æ–‡æœ¬ï¼š"{text}"
```

**è½»æ¾çš„å¼€å‘è€…èŠå¤©ï¼š**

```
ä½ æ˜¯ä¸€ä½è½¯ä»¶å¼€å‘è€…ã€‚
å°†æ–‡æœ¬ç¿»è¯‘/ä¼˜åŒ–ä¸ºåƒ Slack æ¶ˆæ¯ä¸€æ ·çš„è½»æ¾è‹±è¯­ã€‚
æ–‡æœ¬ï¼š"{text}"
```

**æŠ€æœ¯æ–‡æ¡£ï¼š**

```
ä½ æ˜¯ä¸€ä½æŠ€æœ¯æ–‡æ¡£ç¿»è¯‘å‘˜ã€‚
å°†æ–‡æœ¬ç¿»è¯‘/ä¼˜åŒ–ä¸ºç²¾ç¡®çš„æŠ€æœ¯æ–‡æ¡£è‹±è¯­ã€‚
æ–‡æœ¬ï¼š"{text}"
```

æŸ¥çœ‹ `TRANSLATION_PROMPT_EXAMPLE.md` äº†è§£æ›´å¤šç¤ºä¾‹ã€‚

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…

```bash
npm install
```

### 2. è®¾ç½® Ollamaï¼ˆæœ¬åœ° AIï¼‰

```bash
# ä½¿è„šæœ¬å¯æ‰§è¡Œ
chmod +x scripts/setup-ollama.sh

# è¿è¡Œè®¾ç½®ï¼ˆå¯åŠ¨ Ollama å¹¶æ‹‰å–æ¨¡å‹ï¼‰
./scripts/setup-ollama.sh
```

### 3. æ„å»º CLI

```bash
npm run build
```

### 4. å¼€å§‹ä½¿ç”¨

```bash
npm start
# æˆ–
cao
```

## ä½¿ç”¨æ–¹æ³•

### å³æ—¶æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰

åªéœ€è¿è¡Œ CLI å¹¶å¼€å§‹è¾“å…¥ï¼æŒ‰ **Cmd+é”®** å³æ—¶ä¼˜åŒ–ï¼š

```bash
cao
# æˆ–
npm start
```

**å·¥ä½œåŸç†ï¼š**

1. æ­£å¸¸è¾“å…¥æ–‡æœ¬
2. æŒ‰ `Cmd+P` åˆ‡æ¢åˆ°ä¸“ä¸šè¯­æ°”
3. æŒ‰ `Cmd+C` åˆ‡æ¢åˆ°ç®€æ´ç‰ˆæœ¬
4. æŒ‰ `Cmd+G` è¿›è¡Œè¯­æ³•ä¿®æ­£
5. æŒ‰ `Cmd+D` åˆ‡æ¢åˆ°èµ„æ·±å¼€å‘è€…é£æ ¼
6. æŒ‰ **Enter** æ¥å—ä¼˜åŒ–ï¼Œæˆ–ç»§ç»­è¾“å…¥ä»¥å¿½ç•¥

**æ§åˆ¶ï¼š**

- `Ctrl+U` - æ¸…é™¤æ–‡æœ¬
- `Ctrl+C` - é€€å‡º

### ç»å…¸æ¨¡å¼ï¼ˆå¯é€‰ï¼‰

å¦‚æœä½ æ›´å–œæ¬¢å…ˆæäº¤æ–‡æœ¬ï¼Œç„¶åé€‰æ‹©ä¼˜åŒ–ï¼š

```bash
cao --classic
# æˆ–
npm start -- --classic
```

1. è¾“å…¥æ–‡æœ¬ï¼ˆæŒ‰ä¸¤æ¬¡ Enter å®Œæˆï¼‰
2. æŒ‰å¿«æ·é”®æŸ¥çœ‹ä¸åŒçš„ä¼˜åŒ–ï¼š
   - `Ctrl+P` - ä¸“ä¸šè¯­æ°”
   - `Ctrl+C` - ç®€æ´ç‰ˆæœ¬
   - `Ctrl+G` - è¯­æ³•ä¿®æ­£
   - `Ctrl+D` - èµ„æ·±å¼€å‘è€…é£æ ¼
   - `Ctrl+R` - é‡ç½®ä¸ºåŸå§‹æ–‡æœ¬
   - `Ctrl+H` - æŸ¥çœ‹å†å²
   - `Ctrl+Q` - é€€å‡º

### æŸ¥çœ‹å†å²

```bash
cao history
cao history -n 5  # æ˜¾ç¤ºæœ€è¿‘ 5 æ¡è®°å½•
```

### åˆ—å‡ºè‡ªå®šä¹‰æç¤ºè¯

```bash
cao prompts
```

### æ£€æŸ¥é…ç½®

```bash
# æ˜¾ç¤ºå½“å‰é…ç½®
cao config

# è¿è¡Œäº¤äº’å¼è®¾ç½®å‘å¯¼
cao config --setup

# æµ‹è¯• API é…ç½®
cao test
```

æµ‹è¯•å‘½ä»¤å°†ï¼š

- éªŒè¯ä½ çš„ API é…ç½®æ˜¯å¦æœ‰æ•ˆ
- æµ‹è¯•ä¸ API æä¾›å•†çš„è¿æ¥
- è¿è¡Œç®€å•çš„ä¼˜åŒ–æµ‹è¯•
- å¦‚æœå‡ºç°é—®é¢˜ï¼Œæ˜¾ç¤ºè¯¦ç»†çš„é”™è¯¯æ¶ˆæ¯

è®¾ç½®å‘å¯¼å°†å¼•å¯¼ä½ å®Œæˆï¼š

- é€‰æ‹© AI æä¾›å•†ï¼ˆæœ¬åœ° Ollama æˆ–äº‘ç«¯ APIï¼‰
- é…ç½® API è®¾ç½®
- è®¾ç½®é¦–é€‰é¡¹

## é…ç½®

CLI å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼é…ç½®ï¼š

1. **ç¯å¢ƒå˜é‡**ï¼ˆ`.env` æ–‡ä»¶ï¼‰
2. **é…ç½®æ–‡ä»¶**ï¼ˆ`~/.english-optimizer/config.yaml` æˆ– `~/.english-optimizer/config.json`ï¼‰

### ä½¿ç”¨äº‘ç«¯ APIï¼ˆOpenAIã€GLMã€DeepSeek ç­‰ï¼‰

é»˜è®¤æƒ…å†µä¸‹ï¼ŒCLI ä½¿ç”¨æœ¬åœ° Ollamaã€‚è¦ä½¿ç”¨äº‘ç«¯ APIï¼Œä½ éœ€è¦é…ç½®å®ƒä»¬ã€‚

**å¿«é€Ÿè®¾ç½®ï¼š**

1. å¤åˆ¶ç¤ºä¾‹ç¯å¢ƒæ–‡ä»¶ï¼š

```bash
cp .env.example .env
```

2. ä½¿ç”¨ä½ çš„ API é…ç½®ç¼–è¾‘ `.env`ï¼š

```env
AI_PROVIDER=api
API_PROVIDER=openai  # æˆ– 'glm'ã€'custom'
API_KEY=your_api_key_here
API_BASE_URL=https://api.openai.com/v1
API_MODEL=gpt-4o
```

è¯¦ç»†çš„ API é…ç½®è¯´æ˜ï¼Œè¯·å‚è§ [API_SETUP.md](API_SETUP.md)

### ç¯å¢ƒå˜é‡

å¤åˆ¶ `.env.example` åˆ° `.env`ï¼š

```bash
cp .env.example .env
```

ç¼–è¾‘ `.env`ï¼š

```env
# AI æä¾›å•†ï¼šæœ¬åœ°ä½¿ç”¨ 'ollama'ï¼Œäº‘ç«¯ä½¿ç”¨ 'api'
AI_PROVIDER=ollama

# Ollama é…ç½®ï¼ˆæœ¬åœ°ï¼‰
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b

# API é…ç½®ï¼ˆäº‘ç«¯ - OpenAI/GLMï¼‰
# AI_PROVIDER=api
# API_PROVIDER=openai
# API_KEY=your_api_key_here
# API_BASE_URL=https://api.openai.com/v1
# API_MODEL=gpt-3.5-turbo

# GLMï¼ˆæ™ºè°± AIï¼‰
# API_PROVIDER=glm
# API_KEY=your_glm_api_key
# API_BASE_URL=https://open.bigmodel.cn/api/paas/v4
# API_MODEL=glm-4
```

### é…ç½®æ–‡ä»¶

é…ç½®æ–‡ä»¶å¯ä»¥æ˜¯ YAML æˆ– JSON æ ¼å¼ï¼Œä¼šè‡ªåŠ¨åˆ›å»ºåœ¨ `~/.english-optimizer/config.yaml`ï¼ˆæˆ– `config.json`ï¼‰ï¼š

```yaml
ai:
  provider: 'ollama'
  ollama:
    baseUrl: 'http://localhost:11434'
    model: 'llama3.2:3b'
  api:
    provider: 'openai'
    apiKey: ''
    baseUrl: 'https://api.openai.com/v1'
    model: 'gpt-3.5-turbo'

hotkeys:
  professional: 'p'
  concise: 'c'
  grammar: 'g'
  senior_developer: 'd'
  reset: 'r'
  history: 'h'
  quit: 'q'

features:
  enableHistory: true
  historyPath: ~/.english-optimizer/history.json
  enableCustomPrompts: true
  customPromptsPath: ~/.english-optimizer/prompts.json
  useYAMLPrompt: true
```

## è‡ªå®šä¹‰æç¤ºè¯

ä½ å¯ä»¥åœ¨ `~/.english-optimizer/prompts.json` åˆ›å»ºè‡ªå®šä¹‰ä¼˜åŒ–æç¤ºè¯ï¼š

```json
[
  {
    "name": "Academic",
    "description": "ä»¥é€‚åˆç ”ç©¶è®ºæ–‡çš„å­¦æœ¯é£æ ¼é‡å†™",
    "prompt": "è¯·ä»¥å­¦æœ¯é£æ ¼é‡å†™ä»¥ä¸‹æ–‡æœ¬...",
    "hotkey": "a"
  },
  {
    "name": "Friendly",
    "description": "ä½¿æ–‡æœ¬æ›´åŠ å‹å¥½å’Œéšæ„",
    "prompt": "è¯·é‡å†™ä»¥ä¸‹æ–‡æœ¬ä½¿å…¶å¬èµ·æ¥æ›´å‹å¥½...",
    "hotkey": "f"
  }
]
```

## Docker å‘½ä»¤

```bash
# å¯åŠ¨ Ollama
docker-compose up -d

# åœæ­¢ Ollama
docker-compose down

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f ollama

# é‡å¯ Ollama
docker-compose restart
```

## ç”Ÿäº§éƒ¨ç½²

ä½¿ç”¨äº‘ç«¯ API è€Œä¸æ˜¯æœ¬åœ° Ollamaï¼š

1. æ›´æ–°ä½ çš„ `.env` æ–‡ä»¶ï¼š

   ```env
   AI_PROVIDER=api
   API_PROVIDER=openai  # æˆ– 'glm' ç”¨äºæ™ºè°± AI
   API_KEY=your_actual_api_key
   API_MODEL=gpt-3.5-turbo  # æˆ–ä½ å–œæ¬¢çš„æ¨¡å‹
   ```

2. CLI å°†è‡ªåŠ¨ä½¿ç”¨äº‘ç«¯ API

## å¼€å‘

```bash
# ç›‘è§†å¹¶é‡æ–°æ„å»º
npm run watch

# åœ¨å¼€å‘æ¨¡å¼ä¸‹è¿è¡Œ
npm run dev

# ä»£ç æ£€æŸ¥
npm run lint
```

## é¡¹ç›®ç»“æ„

```
english-optimizer-cli/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts              # CLI å…¥å£ç‚¹
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ editor.ts         # å¸¦å¿«æ·é”®çš„äº¤äº’å¼ç¼–è¾‘å™¨
â”‚   â”‚   â””â”€â”€ optimizer.ts      # ä¼˜åŒ–ç¼–æ’
â”‚   â”œâ”€â”€ ai/
â”‚   â”‚   â”œâ”€â”€ provider.ts       # AI æä¾›å•†å·¥å‚
â”‚   â”‚   â”œâ”€â”€ ollama.ts         # Ollama é›†æˆ
â”‚   â”‚   â””â”€â”€ api-provider.ts   # OpenAI/GLM é›†æˆ
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”œâ”€â”€ templates.ts      # å†…ç½®æç¤ºè¯æ¨¡æ¿
â”‚   â”‚   â””â”€â”€ custom.ts         # è‡ªå®šä¹‰æç¤ºè¯åŠ è½½å™¨
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ config.ts         # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ history/
â”‚   â”‚   â””â”€â”€ logger.ts         # å†å²è®°å½•
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ display.ts        # è¾“å‡ºæ ¼å¼åŒ–
â”œâ”€â”€ docker-compose.yml        # Ollama Docker è®¾ç½®
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ setup-ollama.sh      # è®¾ç½®è„šæœ¬
â””â”€â”€ package.json
```

## æ•…éšœæ’é™¤

### Ollama è¿æ¥å¤±è´¥

```bash
# æ£€æŸ¥ Ollama æ˜¯å¦è¿è¡Œ
docker-compose ps

# é‡å¯ Ollama
docker-compose restart

# æ£€æŸ¥ Ollama æ—¥å¿—
docker-compose logs ollama
```

### æœªæ‰¾åˆ°æ¨¡å‹

```bash
# æ‰‹åŠ¨æ‹‰å–æ¨¡å‹
docker exec -it english-optimizer-ollama ollama pull llama3.2:3b
```

### API å¯†é’¥é—®é¢˜

ç¡®ä¿ä½ çš„ `.env` æ–‡ä»¶æœ‰æ­£ç¡®çš„ API å¯†é’¥ï¼Œä¸” API æä¾›å•† URL å‡†ç¡®ã€‚

## è®¸å¯è¯

MIT

## è´¡çŒ®

æ¬¢è¿è´¡çŒ®ï¼è¯·éšæ—¶æäº¤ Pull Requestã€‚

## ä½œè€…

ä¸ºä¸–ç•Œå„åœ°çš„éè‹±è¯­æ¯è¯­è€…ç”¨ â¤ï¸ åˆ›å»ºã€‚
